{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens(article_index):\n",
    "    with open(str(article_index) + \".txt\") as fp: \n",
    "        lines = fp.read().splitlines()\n",
    "    text = \"\"\n",
    "    text = \" \".join(lines)\n",
    "    \n",
    "    # (1)Tokenization\n",
    "    # remove the punctuations while making tokens\n",
    "    tokenizer = RegexpTokenizer(r'\\w+') \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # (2)Lowercasing everything：\n",
    "    tokens_lower = [w.lower() for w in tokens]\n",
    "\n",
    "    # (3)Stemming using Porter’s algorithm\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    tokens_stemmed = [porter_stemmer.stem(w) for w in tokens_lower]\n",
    "\n",
    "\n",
    "    # (4)Stopwords removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens_filtered = []   \n",
    "    for w in tokens_stemmed: \n",
    "        if w not in stop_words: \n",
    "            tokens_filtered.append(w) \n",
    "\n",
    "    return tokens_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vector(article_index, word_list, dictionary):\n",
    "    docu_tokens = tokens(article_index)\n",
    "    term_count = 0\n",
    "    vec_length_square = 0.0\n",
    "    vec = []\n",
    "    for word in sorted(set(docu_tokens)):\n",
    "        if word in word_list:\n",
    "            \n",
    "            term_count += 1\n",
    "            # calculate the square of the length the tfidf vector\n",
    "            vec_length_square += (docu_tokens.count(word) * math.log10(N / dictionary[word]))**2\n",
    "            vec.append([word_list.index(word) + 1, docu_tokens.count(word) * math.log10(N / dictionary[word])])\n",
    "            \n",
    "    return term_count, vec, vec_length_square**(0.5) # length it self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_unit_vector_output(article_index, word_list, dictionary):\n",
    "    term_count, vector, length = tfidf_vector(article_index, word_list, dictionary)\n",
    "    for i in range(len(vector)):\n",
    "        \n",
    "        # devide every entry by the length of the vector to create unit vector\n",
    "        vector[i][1] = (vector[i][1] / length)\n",
    "        \n",
    "    with open(str(article_index)+'_unit.txt', 'w') as file:\n",
    "        file.write('%-5s\\n' %term_count)\n",
    "        file.write('%-5s\\t%-15s\\t\\n' %('t_index', 'tfidf'))\n",
    "        for t_index, tfidf in vector:\n",
    "            file.write('%-5d\\t%-15f\\n' %(t_index, tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_unit_vector(article_index, word_list, dictionary):\n",
    "    term_count, vector, length = tfidf_vector(article_index, word_list, dictionary)\n",
    "    for i in range(len(vector)):\n",
    "        vector[i][1] = (vector[i][1] / length)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(doc_x, doc_y):\n",
    "    vec_x = tfidf_unit_vector(doc_x, word_list, dictionary)\n",
    "    vec_y = tfidf_unit_vector(doc_y, word_list, dictionary)\n",
    "    \n",
    "    p_x = 0\n",
    "    p_y = 0\n",
    "    \n",
    "    cosine_result = 0.0\n",
    "    \n",
    "    # only do inner poduct if two documents have the same t_index\n",
    "    # else their inner product will be zero since tf of one document is zero\n",
    "    while p_x < len(vec_x) and p_y < len(vec_y):\n",
    "        if vec_x[p_x][0] == vec_y[p_y][0]:\n",
    "            cosine_result += vec_x[p_x][1] * vec_y[p_y][1]\n",
    "            p_x += 1\n",
    "            p_y += 1\n",
    "            \n",
    "        elif vec_x[p_x][0] < vec_y[p_y][0]:\n",
    "            p_x += 1\n",
    "        else:\n",
    "            p_y += 1\n",
    "    return round(cosine_result, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasNumbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus size\n",
    "N = 1095"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_all = []\n",
    "for i in range(1, N+1):\n",
    "    tokens_all.extend(set(tokens(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_all.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {}\n",
    "for word, df in Counter(tokens_all).items():\n",
    "    # if df equals one, inner product will be zero while calculating cosine similarity\n",
    "    # since it appears in only one document. So I will eliminate that term\n",
    "    \n",
    "    if df > 1 and len(word) > 1:\n",
    "        if (hasNumbers(word)) or ('_' in word):\n",
    "\n",
    "            # if the number can be recognized as a year, like 19xx or 20xx, then save it as a term        \n",
    "            if  (len(word) is 4) and ((word[0] is '1' and word[1] is '9') or (word[0] is '2' and word[1] is '0')):\n",
    "                dictionary[word] = df\n",
    "        else:\n",
    "            dictionary[word] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('IR_hw2_dictionary.txt', 'w') as file:\n",
    "    file.write('%-5s\\t%-8s\\t%-4s\\n' %('t_index', 'term', 'df'))\n",
    "    \n",
    "    for i,word, df in zip(range(1, len(dictionary)+1), dictionary.keys(), dictionary.values()): # t_index, term, df\n",
    "        file.write('%-5d\\t%-8s\\t%-4d\\n' %(i, word, df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = list(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_unit_vector_output(1, word_list, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
